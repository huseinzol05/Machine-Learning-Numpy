{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import time\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(file, lower = False):\n",
    "    with open(file, 'r') as fopen:\n",
    "        data = fopen.read()\n",
    "    if lower:\n",
    "        data = data.lower()\n",
    "    vocab = list(set(data))\n",
    "    return data, vocab\n",
    "\n",
    "def embed_to_onehot(data, vocab):\n",
    "    onehot = np.zeros((len(data), len(vocab)), dtype = np.float32)\n",
    "    for i in range(len(data)):\n",
    "        onehot[i, vocab.index(data[i])] = 1.0\n",
    "    return onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text, text_vocab = get_vocab('consumer.h', lower = False)\n",
    "onehot = embed_to_onehot(text, text_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "batch_size = 64\n",
    "sequence_length = 12\n",
    "epoch = 1000\n",
    "num_layers = 2\n",
    "size_layer = 128\n",
    "possible_batch_id = range(len(text) - sequence_length - 1)\n",
    "dimension = onehot.shape[1]\n",
    "\n",
    "U = np.random.randn(size_layer, dimension) / np.sqrt(size_layer)\n",
    "Wf = np.random.randn(size_layer, size_layer) / np.sqrt(size_layer)\n",
    "Wi = np.random.randn(size_layer, size_layer) / np.sqrt(size_layer)\n",
    "Wc = np.random.randn(size_layer, size_layer) / np.sqrt(size_layer)\n",
    "Wo = np.random.randn(size_layer, size_layer) / np.sqrt(size_layer)\n",
    "V = np.random.randn(dimension, size_layer) / np.sqrt(dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x, grad=False):\n",
    "    if grad:\n",
    "        output = np.tanh(x)\n",
    "        return (1.0 - np.square(output))\n",
    "    else:\n",
    "        return np.tanh(x)\n",
    "    \n",
    "def sigmoid(x, grad=False):\n",
    "    if grad:\n",
    "        return sigmoid(x) * (1 - sigmoid(x))\n",
    "    else:\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "def softmax(x):\n",
    "    exp_scores = np.exp(x - np.max(x))\n",
    "    return exp_scores / (np.sum(exp_scores, axis=1, keepdims=True) + 1e-8)\n",
    "\n",
    "def derivative_softmax_cross_entropy(x, y):\n",
    "    delta = softmax(x)\n",
    "    delta[range(X.shape[0]), y] -= 1\n",
    "    return delta\n",
    "\n",
    "def forward_multiply_gate(w, x):\n",
    "    return np.dot(w, x)\n",
    "\n",
    "def backward_multiply_gate(w, x, dz):\n",
    "    dW = np.dot(dz.T, x)\n",
    "    dx = np.dot(w.T, dz.T)\n",
    "    return dW, dx\n",
    "\n",
    "def forward_add_gate(x1, x2):\n",
    "    return x1 + x2\n",
    "\n",
    "def backward_add_gate(x1, x2, dz):\n",
    "    dx1 = dz * np.ones_like(x1)\n",
    "    dx2 = dz * np.ones_like(x2)\n",
    "    return dx1, dx2\n",
    "\n",
    "def cross_entropy(Y_hat, Y, epsilon=1e-12):\n",
    "    Y_hat = np.clip(Y_hat, epsilon, 1. - epsilon)\n",
    "    N = Y_hat.shape[0]\n",
    "    return -np.sum(np.sum(Y * np.log(Y_hat+1e-9))) / N\n",
    "\n",
    "def forward_recurrent(x, c_state, h_state, U, Wf, Wi, Wc, Wo, V):\n",
    "    mul_u = forward_multiply_gate(x, U.T)\n",
    "    mul_Wf = forward_multiply_gate(h_state, Wf.T)\n",
    "    add_Wf = forward_add_gate(mul_u, mul_Wf)\n",
    "    f = sigmoid(add_Wf)\n",
    "    mul_Wi = forward_multiply_gate(h_state, Wi.T)\n",
    "    add_Wi = forward_add_gate(mul_u, mul_Wi)\n",
    "    i = sigmoid(add_Wi)\n",
    "    mul_Wc = forward_multiply_gate(h_state, Wc.T)\n",
    "    add_Wc = forward_add_gate(mul_u, mul_Wc)\n",
    "    c_hat = tanh(add_Wc)\n",
    "    C = c_state * f + i * c_hat\n",
    "    mul_Wo = forward_multiply_gate(h_state, Wo.T)\n",
    "    add_Wo = forward_add_gate(mul_u, mul_Wo)\n",
    "    o = sigmoid(add_Wo)\n",
    "    h = o * tanh(C)\n",
    "    mul_v = forward_multiply_gate(h, V.T)\n",
    "    return (mul_u, mul_Wf, add_Wf, mul_Wi, add_Wi, mul_Wc, add_Wc, C, mul_Wo, add_Wo, h, mul_v, i, o, c_hat)\n",
    "\n",
    "def backward_recurrent(x, c_state, h_state, U, Wf, Wi, Wc, Wo, V, d_mul_v, saved_graph):\n",
    "    mul_u, mul_Wf, add_Wf, mul_Wi, add_Wi, mul_Wc, add_Wc, C, mul_Wo, add_Wo, h, mul_v, i, o, c_hat = saved_graph\n",
    "    dV, dh = backward_multiply_gate(V, h, d_mul_v)\n",
    "    dC = tanh(C, True) * o * dh.T\n",
    "    do = tanh(C) * dh.T\n",
    "    dadd_Wo = sigmoid(add_Wo, True) * do\n",
    "    dmul_u1, dmul_Wo = backward_add_gate(mul_u, mul_Wo, dadd_Wo)\n",
    "    dWo, dprev_state = backward_multiply_gate(Wo, h_state, dmul_Wo)\n",
    "    dc_hat = dC * i\n",
    "    dadd_Wc = tanh(add_Wc, True) * dc_hat\n",
    "    dmul_u2, dmul_Wc = backward_add_gate(mul_u, mul_Wc, dadd_Wc)\n",
    "    dWc, dprev_state = backward_multiply_gate(Wc, h_state, dmul_Wc)\n",
    "    di = dC * c_hat\n",
    "    dadd_Wi = sigmoid(add_Wi, True) * di\n",
    "    dmul_u3, dmul_Wi = backward_add_gate(mul_u, mul_Wi, dadd_Wi)\n",
    "    dWi, dprev_state = backward_multiply_gate(Wi, h_state, dmul_Wi)\n",
    "    df = dC * c_state\n",
    "    dadd_Wf = sigmoid(add_Wf, True) * df\n",
    "    dmul_u4, dmul_Wf = backward_add_gate(mul_u, mul_Wf, dadd_Wf)\n",
    "    dWf, dprev_state = backward_multiply_gate(Wf, h_state, dmul_Wf)\n",
    "    dU, dx = backward_multiply_gate(U, x, dmul_u4)\n",
    "    return (dU, dWf, dWi, dWc, dWo, dV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50, loss 4.309586, accuracy 0.029948\n",
      "epoch 100, loss 4.305436, accuracy 0.023438\n",
      "epoch 150, loss 4.301981, accuracy 0.048177\n",
      "epoch 200, loss 4.299302, accuracy 0.050781\n",
      "epoch 250, loss 4.291727, accuracy 0.082031\n",
      "epoch 300, loss 4.289577, accuracy 0.080729\n",
      "epoch 350, loss 4.287231, accuracy 0.097656\n",
      "epoch 400, loss 4.282331, accuracy 0.118490\n",
      "epoch 450, loss 4.272729, accuracy 0.162760\n",
      "epoch 500, loss 4.268828, accuracy 0.167969\n",
      "epoch 550, loss 4.259680, accuracy 0.178385\n",
      "epoch 600, loss 4.255541, accuracy 0.179688\n",
      "epoch 650, loss 4.254507, accuracy 0.183594\n",
      "epoch 700, loss 4.237344, accuracy 0.197917\n",
      "epoch 750, loss 4.236598, accuracy 0.197917\n",
      "epoch 800, loss 4.238123, accuracy 0.178385\n",
      "epoch 850, loss 4.234368, accuracy 0.154948\n",
      "epoch 900, loss 4.227672, accuracy 0.158854\n",
      "epoch 950, loss 4.220145, accuracy 0.164062\n",
      "epoch 1000, loss 4.217878, accuracy 0.167969\n"
     ]
    }
   ],
   "source": [
    "for i in range(epoch):\n",
    "    batch_x = np.zeros((batch_size, sequence_length, dimension))\n",
    "    batch_y = np.zeros((batch_size, sequence_length, dimension))\n",
    "    batch_id = random.sample(possible_batch_id, batch_size)\n",
    "    prev_c = np.zeros((batch_size, size_layer))\n",
    "    prev_h = np.zeros((batch_size, size_layer))\n",
    "    for n in range(sequence_length):\n",
    "        id1 = [k + n for k in batch_id]\n",
    "        id2 = [k + n + 1 for k in batch_id]\n",
    "        batch_x[:,n,:] = onehot[id1, :]\n",
    "        batch_y[:,n,:] = onehot[id2, :]\n",
    "    layers = []\n",
    "    out_logits = np.zeros((batch_size, sequence_length, dimension))\n",
    "    for n in range(sequence_length):\n",
    "        layers.append(forward_recurrent(batch_x[:,n,:], prev_c, prev_h, U, Wf, Wi, Wc, Wo, V))\n",
    "        prev_c = layers[-1][7]\n",
    "        prev_h = layers[-1][10]\n",
    "        out_logits[:, n, :] = layers[-1][-4]\n",
    "    probs = softmax(out_logits.reshape((-1, dimension)))\n",
    "    y = np.argmax(batch_y.reshape((-1, dimension)),axis=1)\n",
    "    accuracy = np.mean(np.argmax(probs,axis=1) == y)\n",
    "    loss = cross_entropy(probs, batch_y.reshape((-1, dimension)))\n",
    "    delta = probs\n",
    "    delta[range(y.shape[0]), y] -= 1\n",
    "    delta = delta.reshape((batch_size, sequence_length, dimension))\n",
    "    dU = np.zeros(U.shape)\n",
    "    dV = np.zeros(V.shape)\n",
    "    dWf = np.zeros(Wf.shape)\n",
    "    dWi = np.zeros(Wi.shape)\n",
    "    dWc = np.zeros(Wc.shape)\n",
    "    dWo = np.zeros(Wo.shape)\n",
    "    prev_c = np.zeros((batch_size, size_layer))\n",
    "    prev_h = np.zeros((batch_size, size_layer))\n",
    "    for n in range(sequence_length):\n",
    "        d_mul_v = delta[:, n, :]\n",
    "        dU_t, dWf_t, dWi_t, dWc_t, dWo_t, dV_t = backward_recurrent(batch_x[:,n,:], prev_c, prev_h, U, Wf, Wi, \n",
    "                                                                    Wc, Wo, V, d_mul_v, layers[n])\n",
    "        prev_c = layers[n][7]\n",
    "        prev_h = layers[n][10]\n",
    "        dU += dU_t\n",
    "        dV += dV_t\n",
    "        dWf += dWf_t\n",
    "        dWi += dWi_t\n",
    "        dWc += dWc_t\n",
    "        dWo += dWo_t\n",
    "    U -= learning_rate * dU\n",
    "    V -= learning_rate * dV\n",
    "    Wf -= learning_rate * dWf\n",
    "    Wi -= learning_rate * dWi\n",
    "    Wc -= learning_rate * dWc\n",
    "    Wo -= learning_rate * dWo\n",
    "    if (i+1) % 50 == 0:\n",
    "        print('epoch %d, loss %f, accuracy %f'%(i+1, loss, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
